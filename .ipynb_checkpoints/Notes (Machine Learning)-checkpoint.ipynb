{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Search\n",
    "\n",
    "## Exact Solution\n",
    "\n",
    "- Given Average Gradient: $\\nabla\\mathcal L_n(\\theta)$\n",
    "- Calculate solution(s) for $\\nabla\\mathcal L_n(\\theta)=0$\n",
    "- Find solution with $\\theta$, that returns training loss (Average Loss): $\\mathcal L_n(\\theta;S_n)$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "- Start with random $\\theta$\n",
    "- Update $\\theta\\leftarrow\\theta-\\eta_k\\nabla\\mathcal L_n(\\theta)$\n",
    "    - $\\eta_k$ is the learning rate\n",
    "    - $k$ is the iteration number\n",
    "- Repeat step 2 until the change in $\\mathcal L_n(\\theta)$ is below a *small* threshold (convergence)\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "- Start with random $\\theta$\n",
    "- Update $\\theta\\leftarrow\\theta-\\eta_k\\nabla\\mathcal L_m(\\theta;\\mathcal B_m)$\n",
    "    - $\\mathcal B_m$ is a random sample of $S_n$ (minibatch) \n",
    "- Repeat step 2 until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### Problem: \n",
    "- Input: matrix x\n",
    "$$\n",
    "x=\\left(\\begin{array}{ccc}x_{11}&x_{21}&...&x_{d1}\\\\x_{12}&x_{22}&...&x_{d2}\\\\...&...&...&...\\\\x_{1n}&x_{2n}&...&x_{dn}\\end{array}\\right)\n",
    "$$\n",
    "    - $n$ is the number of observations\n",
    "    - $d$ is the number of features\n",
    "- Ouput: vector $y:\\{y_1, y_2, ... , y_n\\}$\n",
    "- x and y can be any numerical value\n",
    "- $x\\in\\mathbb R^d, y\\in\\mathbb R$\n",
    "\n",
    "### Model:\n",
    "- $f(x;\\theta, \\theta_0)=\\theta_1x_1+\\theta_2x_2+...+\\theta_dx_d+\\theta_0$\n",
    "- $f(x;\\theta, \\theta_0)=\\theta^Tx+\\theta_0$\n",
    "- parameters: $\\theta\\in\\mathbb R^d,\\theta_0\\in\\mathbb R$\n",
    "\n",
    "Optimization (**General**):\n",
    "- Loss function: $\\text{Loss}(z)=\\frac{1}{2}z^2$\n",
    "- Point loss: $\\mathcal L_1(\\theta;x,y)=Loss(y-f(x;\\theta))$\n",
    "- Average loss: $\\mathcal L_n(\\theta;S_n)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\frac{1}{2}(y-f(x;\\theta))^2$\n",
    "- Point gradient: $\\nabla\\mathcal L_1(\\theta;x,y)=\\frac{d}{d\\theta}\\mathcal L_1(\\theta;x,y)$\n",
    "- Average gradien: $\\nabla\\mathcal L_n(\\theta;S_n) = \\frac{1}{n}\\sum_{(x,y)\\in S_n}\\nabla\\mathcal L_1(\\theta;x,y)$\n",
    "\n",
    "### Model (**Constant Feature Trick**):\n",
    "- $f(x;\\theta,\\theta_0)=\\theta_1x_1+\\theta_2x_2+...+\\theta_dx_d+\\boldsymbol{\\theta_0x_0}$\n",
    "    - $x_0 = \\mathbb1$ \n",
    "- $f(x;\\tilde\\theta)=\\tilde\\theta^T\\tilde x$\n",
    "    - $\\tilde\\theta=(\\theta,\\theta_0)$\n",
    "    - $\\tilde x=(x,x_0)$\n",
    "- parameters: $\\tilde\\theta\\in\\mathbb R^{d+1},\\tilde x\\in\\mathbb R^{d+1}$\n",
    "\n",
    "### Optimization\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x\\leftarrow\\tilde x\\\\\n",
    "\\theta\\leftarrow\\tilde\\theta\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Point loss: $\\mathcal L_1(\\theta;x,y)=\\frac{1}{2}(y-\\theta^Tx)$\n",
    "- Average loss: $\\mathcal L_n(\\theta;S_n)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\frac{1}{2}(y-\\theta^Tx)^2$\n",
    "- Point gradient: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla\\mathcal L_1(\\theta;x,y)&=\\frac{d}{d\\theta}\\frac{1}{2}(y-\\theta^Tx)^2\\\\\n",
    "&=\\frac{d}{d\\theta}\\left(\\begin{array}{c}\\frac{1}{2}(y-\\theta^Tx_1)^2\\\\\\frac{1}{2}(y-\\theta^Tx_2)^2\\\\...\\\\\\frac{1}{2}(y-\\theta^Tx_d)^2\\end{array}\\right)\\\\\n",
    "&=\\left(\\begin{array}{c}-x_1(y-\\theta^Tx_1)\\\\-x_2(y-\\theta^Tx_2)\\\\...\\\\-x_d(y-\\theta^Tx_d)\\end{array}\\right)\\\\\n",
    "&=-\\left(\\begin{array}{c}x_1\\\\x_2\\\\...\\\\x_d\\end{array}\\right)(y-\\theta^Tx)\\\\\n",
    "&=-x(y-\\theta^Tx)\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Average gradient:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla\\mathcal L_n(\\theta)&=\\frac{1}{n}\\sum_{(x,y)\\in S_n}-x(y-\\theta^Tx)\\\\\n",
    "&= \\frac{1}{n}(-x_1(y_1-\\theta^Tx_1)-x_2(y_2-\\theta^Tx_2)+...-x_n(y_n-\\theta^Tx_n))\\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^n-x_iy_i+x_ix_i^T\\theta\\\\\n",
    "&=\\frac{1}{n}(X^TX\\theta -X^TY)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Solution:\n",
    "- Exact solution: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{n}(X^TX\\hat\\theta -X^TY) &= 0\\\\\n",
    "X^TX\\hat\\theta &=X^TY\\\\\n",
    "\\hat\\theta&=(X^TX)^{-1}X^TY\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Gradient Descent: $\\theta\\leftarrow\\theta-\\eta_k\\left[\\frac{1}{n}(X^TX\\theta-X^TY)\\right]$\n",
    "\n",
    "### Test Loss (**Actual Data**):\n",
    "\n",
    " $\\mathcal R(\\theta)=\\frac{1}{n}\\sum_{x,y}\\frac{1}{2}(y-\\theta^Tx)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "### Problem:\n",
    "Same as Linear Regression\n",
    "\n",
    "### Model:\n",
    "Same as Linear Regression with Constant Feature Trick\n",
    "\n",
    "### Optimization:\n",
    "- Point loss: $\\mathcal L_{1,\\lambda}(\\theta)=\\frac{1}{2}(y-\\theta^Tx)^2\\boldsymbol{+\\frac{\\lambda}{2}{\\left\\lVert\\theta\\right\\rVert}^2}$\n",
    "- Average loss: $\\mathcal L_{n,\\lambda}(\\theta)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\frac{1}{2}(y-\\theta^Tx)^2+\\frac{\\lambda}{2}{\\left\\lVert\\theta\\right\\rVert}^2$\n",
    "- Point gradient:\n",
    "$$\n",
    "\\begin{array}\n",
    "\\nabla\\mathcal L_{1,\\lambda}&= \\frac{d}{d\\theta}(\\frac{1}{2}(y-\\theta^Tx)^2+\\frac{\\lambda}{2}{\\left\\lVert\\theta\\right\\rVert}^2)\\\\\n",
    "&= -x(y-\\theta^Tx)+\\lambda\\theta\n",
    "\\end{array}\n",
    "$$\n",
    "- Average gradint:\n",
    "$$\n",
    "\\begin{array}\n",
    "\\nabla \\mathcal L_{n,\\lambda}&=\\frac{d}{d\\theta}(\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\frac{1}{2}(y-\\theta^Tx)^2+\\frac{\\lambda}{2}{\\left\\lVert\\theta\\right\\rVert}^2)\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=0}^n-x_i(y_i-\\theta^Tx_i)+\\lambda\\theta\\\\\n",
    "&=\\frac{1}{n}(X^TX\\theta-X^TY)+\\lambda\\theta\n",
    "\\end{array}\n",
    "$$\n",
    "### Solution:\n",
    "- Exact solution:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{n}(X^TX\\hat\\theta-X^TY)+\\lambda\\hat\\theta&=0\\\\\n",
    "X^TX\\hat\\theta+n\\lambda\\hat\\theta&=X^TY\\\\\n",
    "\\hat\\theta(X^TX+n\\lambda I)&=X^TY\\\\\n",
    "\\hat\\theta&=(X^TX+n\\lambda I)^{-1}X^TY\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Gradient descent: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta&\\leftarrow\\theta-\\eta_k(\\frac{1}{n}(X^TX\\theta-X^TY)+\\lambda\\theta)\\\\\n",
    "\\theta&\\leftarrow\\theta-\\eta_k(\\frac{1}{n}(X^TX\\theta-X^TY))-\\eta_k\\lambda\\theta\\\\\n",
    "\\theta&\\leftarrow(1-\\eta_k\\lambda)\\theta-\\eta_k(\\frac{1}{n}(X^TX\\theta-X^TY))\n",
    "\\end{aligned}\n",
    "$$\n",
    "*Note: Always Invertible*\n",
    "\n",
    "### Test Loss:\n",
    "\n",
    "$\\mathcal R(\\theta)=\\frac{1}{n}\\sum_{x,y}\\frac{1}{2}(y-\\theta^Tx)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification (Perceptrons)\n",
    "\n",
    "### Problem:\n",
    "\n",
    "- Input: matrix\n",
    "$$\n",
    "x=\\left(\\begin{array}{cccc}x_{11}&x_{21}&...&x_{d1}\\\\x_{12}&x_{22}&...&x_{d2}\\\\...&...&...&...\\\\x_{1n}&x_{2n}&...&x_{dn}\\end{array}\\right)\n",
    "$$\n",
    "- Output: vector $y:\\{y_1,y_2,...,y_d\\}$\n",
    "- x can take any numerical value, $y_i\\in\\{-1,+1\\}$\n",
    "- $x\\in\\mathbb R^d, y\\in\\mathbb R$\n",
    "\n",
    "### Model\n",
    "\n",
    "$$\\begin{aligned}h(x;\\theta,\\theta_0)&=\\text{sign}(\\theta_1x_1+\\theta_2x_2+...+\\theta_dx_d+\\theta_0)\\\\&=\\text{sign}(\\theta_1x_1+\\theta_2x_2+...+\\theta_dx_d+\\theta_0x_0)\\\\&=\\text{sign}(\\tilde\\theta^T\\tilde x)\\end{aligned}$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x&\\leftarrow\\tilde x\\\\\n",
    "\\theta&\\leftarrow\\tilde\\theta\n",
    "\\end{aligned}\n",
    "$$\n",
    "- parameters: $\\theta\\in\\mathbb R^{d+1}$\n",
    "\n",
    "### Optimization:\n",
    "\n",
    "- Loss function: $\\text{Loss}(z)=\\text{Ind}[z\\leq0]$\n",
    "    - $\\text{Ind}[x]=\\left\\{\\begin{array}{ll}1&x=\\text{True}\\\\0&x=\\text{False}\\end{array}\\right.$\n",
    "- Point loss: $\\mathcal L_1(\\theta;x,y)=\\text{Ind}[y(\\theta^Tx)\\leq0]$\n",
    "    - $y\\neq h(x;\\theta)\\ \\ \\Rightarrow\\ \\ y(\\theta^Tx)\\leq0$\n",
    "    - *Note: misclassification and boundary return same results*\n",
    "- Average loss: $\\mathcal L_n(\\theta;S_n)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\mathcal L_1(\\theta;x,y)$\n",
    "- *note: gradient is 0 almost everywhere, gradient descent impossible.*\n",
    "\n",
    "## Solution:\n",
    "\n",
    "**Perceptron Algorithm (Mistake-Driven Algorithm)**\n",
    "- Initialize $\\theta=0$\n",
    "- For each data $(x,y)\\in S_n$\n",
    "    - if $y(\\theta^Tx)\\leq0$, \n",
    "        - $\\theta\\leftarrow\\theta+yx$\n",
    "- Repeat Step 2 until no mistakes are found\n",
    "- *note: algorithm never terminates if not linearly seperable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge Loss (Support Vector Machine)\n",
    "\n",
    "### Problem\n",
    "\n",
    "Same as Linear Classification\n",
    "\n",
    "### Model\n",
    "\n",
    "Same as Linear Classification with Constant Feature Trick\n",
    "\n",
    "### Optimization\n",
    "\n",
    "- Loss function: $\\text{Loss}(z)=\\text{max}\\{1-z,0\\}$\n",
    "- Point loss: $\\mathcal L_1(\\theta)=\\text{max}\\{1-y(\\theta^Tx),0\\}$ \n",
    "- Average loss: $\\mathcal L_n(\\theta)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\text{max}\\{1-y(\\theta^Tx),0\\}$\n",
    "- Point gradient: $\\nabla\\mathcal L_1(\\theta)=\\left\\{\\begin{array}{ll}0&\\text{if }z>1\\\\-yx&\\text{otherwise}\\end{array}\\right.$\n",
    "\n",
    "### Solution\n",
    "\n",
    "**Stochastic Gradient Descent**\n",
    "- Initialize $\\theta = 0$\n",
    "- Select data $(x,y)\\in S_n$ **at random**\n",
    "    - if $y(\\theta^Tx)\\leq1$,\n",
    "        - $\\theta\\leftarrow\\theta+\\eta_kyx$\n",
    "- Repeat Step 2 until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Problem:\n",
    "\n",
    "Same as Linear Classification\n",
    "\n",
    "### Model:\n",
    "\n",
    "- $\\begin{aligned}h(x;\\theta)&=\\mathbb P(y=+1|\\ x)\\\\&=\\text{Sig}(\\theta^Tx)\\end{aligned}$\n",
    "    - $\\text{Sig}(z)=\\frac{1}{1+e^{-z}}$\n",
    "    - $\\text{Sig}(z)\\in[0,1]$\n",
    "- $\\text{Sig}(\\theta^Tx)\\geq0.5\\Rightarrow\\theta^Tx\\geq0$\n",
    "- $\\text{Sig}(\\theta^Tx)<0.5\\Rightarrow\\theta^Tx<0$\n",
    "- *Note: *$\\theta^Tx=0$* is the decision boundary.*\n",
    "- $\\mathbb P(y|x)=\\text{Sig}(y(\\theta^Tx))$ for $y\\in\\{-1,+1\\}$\n",
    "\n",
    "### Optimization:\n",
    "- Loss function: $\\text{Loss}(z)=\\log(1+e^{-z})$\n",
    "- Point loss: $\\mathcal L_1(\\theta;x,y)=\\log(1+e^{-y(\\theta^Tx)})$\n",
    "- Average loss:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal L_n(\\theta;S_n)&=-\\frac{1}{n}\\log\\prod_{(x,y)\\in S_n}\\mathbb P(y|x)\\\\\n",
    "&= -\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\log\\frac{1}{1+e^{-y(\\theta^Tx)}}\\\\\n",
    "&=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\log(1+e^{-y(\\theta^Tx)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Point gradient:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla\\mathcal L_1(\\theta;x,y)&=\\frac{d}{d\\theta}(\\log(1+e^{-y(\\theta^Tx)}))\\\\\n",
    "&=\\frac{-yxe^{-y(\\theta^Tx)}}{1+e^{-y(\\theta^Tx)}}\\\\\n",
    "&=\\frac{-yx}{1+e^{y(\\theta^Tx)}}\\\\\n",
    "&=-yx\\text{Sig}(-y(\\theta^Tx))\\\\\n",
    "&=\\left\\{\\begin{array}{ll}x(\\text{Sig}(\\theta^Tx)-1)&\\text{if }y=+1\\\\x(\\text{Sig}(\\theta^Tx))&\\text{if }y=-1\\end{array}\\right.\\\\\n",
    "&=x(h(x;\\theta)-\\text{Ind}[y=1])\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Average gradient: $\\nabla\\mathcal L_n(\\theta;S_n)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}x(h(x;\\theta)-\\text{Ind}[y=1])$\n",
    "\n",
    "### Solution\n",
    "\n",
    "- Gradient descent: $\\theta\\leftarrow\\theta-\\frac{\\eta_k}{n}\\sum_{(x,y)\\in S_n}x(h(x;\\theta)-\\text{Ind}[y=1])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "### Problem:\n",
    "\n",
    "- Input: matrix\n",
    "$$\n",
    "x=\\left(\\begin{array}{cccc}x_{11}&x_{21}&...&x_{d1}\\\\x_{12}&x_{22}&...&x_{d2}\\\\...&...&...&...\\\\x_{1n}&x_{2n}&...&x_{dn}\\end{array}\\right)\n",
    "$$\n",
    "- Output: vectors $C_1,C2,...,C_k\\subset\\{1,2,...,n\\}$\n",
    "- x can take any numerical value, $C_i$ are mutually exclusive sets of n\n",
    "\n",
    "- $x\\in\\mathbb R^d, C_i\\in\\mathbb R\\ \\ \\forall i$\n",
    "\n",
    "### Optimazation:\n",
    "\n",
    "- Average loss: \n",
    "$\\begin{aligned}\\mathcal L_{n,k}(C_1,...,C_n;S_n)&=\\sum_{j=1}^n\\sum_{i\\in C_j}\\left\\lVert x_i-\\frac{1}{|C_j|}\\sum_{i'\\in C_j}x_{i'}\\right\\rVert^2\\\\\n",
    "\\mathcal L_{n,k}(C_1,...,C_n,z_1,...,z_n;S_n)&=\\sum_{j=1}^n\\sum_{i\\in C_j}\\left\\lVert x_i-z_j\\right\\rVert^2\n",
    "\\end{aligned}$\n",
    "\n",
    "\n",
    "### Solution:\n",
    "\n",
    "**Coordinate Descent**\n",
    "- Initialize centroids $z_1,z_2,...,z_k$\n",
    "- For each $j\\in\\{1,2,...,k\\}$,\n",
    "    - $C_j=\\{\\text{all }i\\text{ such that }x^i\\text{ is closest to }z_j\\}$\n",
    "- For each new cluster,\n",
    "    - $z_j\\leftarrow\\frac{1}{|C_j|}\\sum_{i\\in C_j}x_i$\n",
    "- Repeat Step 2 and Step 3 until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return Later\n",
    "\n",
    "- 02: Momentum\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Introduction\n",
    "\n",
    "Types of Machine Learning\n",
    "- Unsupervised Learning\n",
    "    - Clustering\n",
    "- Reinforcement Learning\n",
    "- Transfer Learning\n",
    "- Supervised Learning\n",
    "    - Regression\n",
    "        - Linear\n",
    "        - Non-Linear\n",
    "    - Classification\n",
    "        - Linear\n",
    "        - Non-Linear\n",
    "        \n",
    "## Heirarchy of Importance in ML\n",
    "\n",
    "1. More Structure\n",
    "2. More Data\n",
    "3. Better Machines\n",
    "4. Better Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Regression\n",
    "\n",
    "- Task:\n",
    "    - $$\\text{find }f:\\mathbb{R}^d\\rightarrow\\mathbb{R}\\text{ such that }y\\approx f(x;\\theta)$$\n",
    "- Experience:\n",
    "    - $$\\text{Training data }(x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)})$$\n",
    "- Performance:\n",
    "    - $$\\text{Prediction error of }y$$\n",
    "    - $$f(x;\\theta)\\text{ on test data - validation}$$\n",
    "    \n",
    "## Methodology\n",
    "\n",
    "### Features\n",
    "- The input factors that are hypothesised to affect the output value(s)\n",
    "- e.g. temperature, genre of music, etc.\n",
    "\n",
    "### Training Data\n",
    "\n",
    "$$\n",
    "S_n=\\{(x^{(i)}, y^{(i)})|i=1,...,n\\}\n",
    "$$\n",
    "- Notation: $$\\begin{aligned}\\text{Training data set: }&S_n\\\\\\text{Test data set: }&S_*\\end{aligned}$$\n",
    "- Features (inputs): $$x^{(i)}=(x_1^{(i)},...,x_d^{(i)})^T \\in\\mathbb{R}^d$$\n",
    "- Response (output): $$y^{(i)}\\in\\mathbb{R}$$\n",
    "\n",
    "### Model (or Hypothesis Class)\n",
    "\n",
    "$$\\begin{aligned}f(x;\\theta,\\theta_0)&=\\theta_dx_d+...+\\theta_1x_1+\\theta_0\\\\&=\\theta^Tx+\\theta_0\\end{aligned}$$\n",
    "- Notation: $$\\mathcal H$$\n",
    "- Set of linear functions mapping onto a single vector: $$f:\\mathbb R^d\\rightarrow \\mathbb R$$\n",
    "- Model parameters:$$\\theta\\in\\mathbb R^d, \\theta_0\\in\\mathbb R$$\n",
    "\n",
    "### Training Loss / Objective\n",
    "$$min.\\ \\ \\mathcal L(f; S_n)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\frac{1}{2}(y-f(x))^2$$\n",
    "- Traning Algorithm: Set gradient to zero and solve equations\n",
    "- Assumption: Test data and training data are idd\n",
    "\n",
    "### Test Loss / Objective\n",
    "\n",
    "$$\\mathcal R(\\hat f;S_*)=\\frac{1}{n}\\sum_{(x,y)\\in S_*}\\frac{1}{2}(y-\\hat f(x))^2$$\n",
    "\n",
    "- To test how well the model generalises\n",
    "- The Test and Training loss can be different (this time it's not)\n",
    "\n",
    "## Goal of Machine Learning\n",
    "\n",
    "**The goal of Machine Learning is to find predictors that GENERALIZE well.**\n",
    "\n",
    "- If not enough data, the model might underfit\n",
    "    - too few features to accurately predict\n",
    "- If the model doesn't generalize, the model will overfit\n",
    "    - too many useless features, overtune model to training data\n",
    "    \n",
    "Finding a model of the right size is called Model Selection.\n",
    "\n",
    "### Optimization\n",
    "\n",
    "- Loss Function: $$\\text{Loss}(z)=\\frac{1}{2}z^2$$\n",
    "\n",
    "    - Squared error, penalizes big errors heavily\n",
    "    - Very important, it's **convex**\n",
    "\n",
    "- Empirical Risk / Training Loss:\n",
    "    - Point Loss: $$\\mathcal L_1(\\theta; x,y)=\\text{Loss}(y-f(x;\\theta))$$\n",
    "    - Average Loss: $$\\begin{aligned}\\mathcal L_n(\\theta; S_n)&=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\mathcal L_1(\\theta; x,y)\\\\&=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\frac{1}{2}(y-f(x;\\theta))^2\\end{aligned}$$\n",
    "\n",
    "- Gradient:\n",
    "$$\\begin{aligned}\\nabla\\mathcal L_n(\\theta;S_n)&=\\left(\\begin{array}{c}\\frac{\\delta\\mathcal L_n}{\\delta\\theta_1}(\\theta;S_n)\\\\\\frac{\\delta\\mathcal L_n}{\\delta\\theta_2}(\\theta;S_n)\\\\\\vdots\\\\\\frac{\\delta\\mathcal L_n}{\\delta\\theta_d}(\\theta;S_n)\\end{array}\\right)\\\\&=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\left(\\begin{array}{c}\\frac{\\delta\\mathcal L_1}{\\delta\\theta_1}(\\theta;x,y)\\\\\\frac{\\delta\\mathcal L_1}{\\delta\\theta_2}(\\theta;x,y)\\\\\\vdots\\\\\\frac{\\delta\\mathcal L_1}{\\delta\\theta_d}(\\theta;x,y)\\end{array}\\right)\\\\&=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\nabla\\mathcal L_1(\\theta;x,y)\\end{aligned}$$\n",
    "\n",
    "### Solutions\n",
    "\n",
    "#### Exact Solution\n",
    "\n",
    "1. Set gradient to zero, solve parameters\n",
    "2. Run through all solutions to find parameters that has worst training loss\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "1. Initialize $\\theta$ randomly\n",
    "2. Update $\\theta\\leftarrow(\\theta-\\eta\\nabla\\mathcal L_n(\\theta))$\n",
    "3. Repeat (2) until convergence\n",
    "    - when improvement in $\\mathcal L_n(\\theta)$ is smaller than a predefined threshold \n",
    "\n",
    "- Gradient Descent may not guarantee optimal solution, there is a possibility the gradient descent converges at a local minima and terminates.\n",
    "    - Multiple runs should be carried out\n",
    "    - For convex equations, there is only global minima\n",
    "\n",
    "##### Sub-Gradients\n",
    "\n",
    "$$ \n",
    "v\\in \\delta f(x)\\\\\n",
    "\\text{such that, }f(y)-f(x)\\geq v^T(y-x),\\ \\forall y\n",
    "$$\n",
    "\n",
    "- **At non-differential points of training objective function**, there is no gradient.\n",
    "    - Sub-gradients are used instead to continue descent\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "$$\n",
    "\\nabla\\mathcal L_n(\\theta;S_n)=\\frac{1}{n}\\nabla\\mathcal L_1(\\theta;x,y)\n",
    "$$\n",
    "- As the data set gets larger, the global average takes longer to compute.\n",
    "\n",
    "We can approximate the gradient by averaging over a smaller minibatch (subset of the training data)\n",
    "$$\\begin{aligned}\\nabla\\mathcal L_n(\\theta;S_n)&\\approx\\nabla\\mathcal L_m(\\theta;B_m)\\\\&=\\frac{1}{m}\\sum_{(x,y)\\in B_m}\\nabla\\mathcal L_1(\\theta; x,y)\\end{aligned}$$\n",
    "\n",
    "\n",
    "1. Intialize $\\theta$ randomly\n",
    "2. Select minibatch $B_m$ of data from $S_n$ at random\n",
    "$$\\theta\\leftarrow\\theta-\\eta_k\\nabla\\mathcal L_m(\\theta;B_m)$$\n",
    "3. Repeat (2) until convergence\n",
    "\n",
    "##### Considerations\n",
    "\n",
    "- Learning Rate\n",
    "    - Small learning rate, converges more accurately\n",
    "    - Large learning rate, converges faster\n",
    "    - We want a learning rate which starts big and ends small\n",
    "    $$\\eta_k=\\frac{1}{k+1}$$\n",
    "- Momentum\n",
    "    - To prevent the minibatch gradient from fluctuating too much,\n",
    "    we take the weighted sum with respect to the previous update.\n",
    "    $$\\begin{aligned}\\theta^{t+1}&=\\theta^{(t)}-\\eta_k\\Delta^{(t)}\\\\\\Delta^{(t)}&=(1-\\epsilon)\\Delta^{(t-1)}+\\epsilon\\nabla\\mathcal L_m(\\theta;B_m)\\end{aligned}$$\n",
    "\n",
    "## Multivariate Linear Regression\n",
    "\n",
    "### Least Squares\n",
    "\n",
    "- Data:\n",
    "$$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(n)},y^{(n)})\\\\x\\in\\mathbb R^d,y\\in\\mathbb R$$\n",
    "- Model:\n",
    "$$\\begin{aligned}f(x;\\theta,\\theta_0)&=\\theta_1x_1+...+\\theta_dx_d+\\theta_0\\\\&=\\theta^Tx+\\theta_0\\\\\\theta\\in\\mathbb R^d&,\\theta_0\\in\\mathbb R \\end{aligned}$$\n",
    "- Training Objective:\n",
    "$$\\begin{aligned}\\mathcal L_1(\\theta,\\theta_0;x,y)&=\\frac{1}{2}(y-(\\theta^Tx+\\theta_0))^2\\\\\\mathcal L_n(\\theta,\\theta_0;S_n)&=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\mathcal L_1(\\theta,\\theta_0;x,y)\\end{aligned}$$\n",
    "\n",
    "### Constant Feature Trick\n",
    "\n",
    "- Data:\n",
    "$$(\\tilde x^{(1)},y^{(1)}),(\\tilde x^{(2)},y^{(2)}),...,(\\tilde x^{(n)},y^{(n)})\\\\\\tilde x\\in\\mathbb R^{d+1},y\\in\\mathbb R$$\n",
    "- Model:\n",
    "$$\\begin{aligned}f(x;\\theta,\\theta_0)&=\\theta_1x_1+...+\\theta_dx_d+\\theta_0x_0\\\\&=\\tilde\\theta^T\\tilde x\\\\\\tilde\\theta&=(\\theta,\\theta_0)\\in\\mathbb R^{d+1}\\end{aligned}$$\n",
    "- Training Objective:\n",
    "$$\\begin{aligned}\\mathcal L_1(\\tilde\\theta;\\tilde x,y)&=\\frac{1}{2}(y-\\tilde\\theta^T\\tilde x)^2\\\\\\mathcal L_n(\\tilde\\theta;S_n)&=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\mathcal L_1(\\tilde\\theta;\\tilde x,y)\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6: Support Vector Machines (SVM)\n",
    "\n",
    "## Legrange Multipliers\n",
    "\n",
    "Constrained Optimization\n",
    "- Method 1 (Dual Porblem):\n",
    "    - Solve dual optimization problem\n",
    "        - Contraints are nicer (easier to do gradient descent)\n",
    "- Method 2 (Exact Solution):\n",
    "    - Solve Lagrangian system of Equations\n",
    "    \n",
    "Equality Contraints:\n",
    "- Problem:\n",
    "    - $$\\text{min }f(x)$$\n",
    "    - $$\\text{s.t. }h_1(x)=0,...,h_l(x)=0$$\n",
    "- Lagrangian:\n",
    "    - $$L(x,\\lambda)=f(x)+\\lambda_1h_1(x)+...+\\lambda_lh_l(x)$$\n",
    "- Example:\n",
    "    - $$\\text{min }f(x)=n_1logx_1+...+n_dlogx_d$$\n",
    "    - $$\\text{s.t. }h(x)=x_1+...+x_d-1=0$$\n",
    "    - $$L(x,\\lambda)=n_1logx_1+...+n_dlogx_d+\\lambda(x_1+...+x_d-1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 7: Deep Learning\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8: Generative Models\n",
    "\n",
    "## Probabilistic Models\n",
    "\n",
    "- Models are not perfectly accurate\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{max.    }&3y-2x_1-x_2\\\\\n",
    "\\text{s.t.    }&y,x_1,x_2\\geq0\\\\\n",
    "&y=4x_1^{\\frac{1}{4}}x_2^{\\frac{1}{2}}\\\\\n",
    "\\text{max.    }&12x_1^{\\frac{1}{4}}x_2^{\\frac{1}{2}}-2x_1-x_2\\\\\n",
    "&3x_1\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLClass]",
   "language": "python",
   "name": "conda-env-MLClass-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
