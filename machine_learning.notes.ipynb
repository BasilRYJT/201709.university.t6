{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Shaowei\n",
    "\n",
    "## Overview\n",
    "\n",
    "- Lesson 1 : Introduction\n",
    "     - Types of Machine Learning (ML)\n",
    "- Lesson 2 : Regression\n",
    "    - Linear Regression\n",
    "    - Gradient Descent\n",
    "    - Multivariate Linear Regression\n",
    "    - Ridge Regression\n",
    "- Lesson 3: Classification\n",
    "    - Linear Regression\n",
    "    - Perceptron Algorithm\n",
    "    - Hinge Loss Algorithm\n",
    "    - Logistic Regression\n",
    "- Lesson 4: Clustering\n",
    "    - K-means\n",
    "- Lesson 5: Recommendation Systems\n",
    "    - K-nearest Neighbours\n",
    "    - Matrix Factorization\n",
    "- Lesson 6: Support Vector Machines (SVM)\n",
    "    - Legrange Multipliers\n",
    "    - Maximum Margins\n",
    "- Lesson 7: Deep Learning\n",
    "    - Feedforward Networks\n",
    "    - Backpropagation\n",
    "    - Autoencoders\n",
    "- Lesson 8: Generative Models\n",
    "    - Maximum Likelihood\n",
    "    - Log Likelihood\n",
    "- Lesson 9: Expectation Maximum\n",
    "    - Expectation Maximum\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Search\n",
    "\n",
    "### Exact Solution\n",
    "\n",
    "- Given Average Gradient: $\\nabla\\mathcal L_n(\\theta)$\n",
    "- Calculate solution(s) for $\\nabla\\mathcal L_n(\\theta)=0$\n",
    "- Find solution with $\\theta$, that returns training loss (Average Loss): $\\mathcal L_n(\\theta;S_n)$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "- Start with random $\\theta$\n",
    "- Update $\\theta\\leftarrow\\theta-\\eta_k\\nabla\\mathcal L_n(\\theta)$\n",
    "    - $\\eta_k$ is the learning rate\n",
    "    - $k$ is the iteration number\n",
    "- Repeat step 2 until the change in $\\mathcal L_n(\\theta)$ is below a *small* threshold (convergence)\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "- Start with random $\\theta$\n",
    "- Update $\\theta\\leftarrow\\theta-\\eta_k\\nabla\\mathcal L_m(\\theta;\\mathcal B_m)$\n",
    "    - $\\mathcal B_m$ is a random sample of $S_n$ (minibatch) \n",
    "- Repeat step 2 until convergence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### Problem: \n",
    "- Input: matrix x\n",
    "$$\n",
    "x=\\left(\\begin{array}{ccc}x_{11}&x_{21}&...&x_{d1}\\\\x_{12}&x_{22}&...&x_{d2}\\\\...&...&...&...\\\\x_{1n}&x_{2n}&...&x_{dn}\\end{array}\\right)\n",
    "$$\n",
    "    - $n$ is the number of observations\n",
    "    - $d$ is the number of features\n",
    "- Ouput: vector $y:\\{y_1, y_2, ... , y_n\\}$\n",
    "- x and y can be any numerical value\n",
    "- $x\\in\\mathbb R^d, y\\in\\mathbb R$\n",
    "\n",
    "### Model:\n",
    "- $f(x;\\theta, \\theta_0)=\\theta_1x_1+\\theta_2x_2+...+\\theta_dx_d+\\theta_0$\n",
    "- $f(x;\\theta, \\theta_0)=\\theta^Tx+\\theta_0$\n",
    "- parameters: $\\theta\\in\\mathbb R^d,\\theta_0\\in\\mathbb R$\n",
    "\n",
    "Optimization (**General**):\n",
    "- Loss function: $\\text{Loss}(z)=\\frac{1}{2}z^2$\n",
    "- Point loss: $\\mathcal L_1(\\theta;x,y)=Loss(y-f(x;\\theta))$\n",
    "- Average loss: $\\mathcal L_n(\\theta;S_n)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\frac{1}{2}(y-f(x;\\theta))^2$\n",
    "- Point gradient: $\\nabla\\mathcal L_1(\\theta;x,y)=\\frac{d}{d\\theta}\\mathcal L_1(\\theta;x,y)$\n",
    "- Average gradien: $\\nabla\\mathcal L_n(\\theta;S_n) = \\frac{1}{n}\\sum_{(x,y)\\in S_n}\\nabla\\mathcal L_1(\\theta;x,y)$\n",
    "\n",
    "### Model (**Constant Feature Trick**):\n",
    "- $f(x;\\theta,\\theta_0)=\\theta_1x_1+\\theta_2x_2+...+\\theta_dx_d+\\boldsymbol{\\theta_0x_0}$\n",
    "    - $x_0 = \\mathbb1$ \n",
    "- $f(x;\\tilde\\theta)=\\tilde\\theta^T\\tilde x$\n",
    "    - $\\tilde\\theta=(\\theta,\\theta_0)$\n",
    "    - $\\tilde x=(x,x_0)$\n",
    "- parameters: $\\tilde\\theta\\in\\mathbb R^{d+1},\\tilde x\\in\\mathbb R^{d+1}$\n",
    "\n",
    "### Optimization\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x\\leftarrow\\tilde x\\\\\n",
    "\\theta\\leftarrow\\tilde\\theta\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Point loss: $\\mathcal L_1(\\theta;x,y)=\\frac{1}{2}(y-\\theta^Tx)$\n",
    "- Average loss: $\\mathcal L_n(\\theta;S_n)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\frac{1}{2}(y-\\theta^Tx)^2$\n",
    "- Point gradient: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla\\mathcal L_1(\\theta;x,y)&=\\frac{d}{d\\theta}\\frac{1}{2}(y-\\theta^Tx)^2\\\\\n",
    "&=\\frac{d}{d\\theta}\\left(\\begin{array}{c}\\frac{1}{2}(y-\\theta^Tx_1)^2\\\\\\frac{1}{2}(y-\\theta^Tx_2)^2\\\\...\\\\\\frac{1}{2}(y-\\theta^Tx_d)^2\\end{array}\\right)\\\\\n",
    "&=\\left(\\begin{array}{c}-x_1(y-\\theta^Tx_1)\\\\-x_2(y-\\theta^Tx_2)\\\\...\\\\-x_d(y-\\theta^Tx_d)\\end{array}\\right)\\\\\n",
    "&=-\\left(\\begin{array}{c}x_1\\\\x_2\\\\...\\\\x_d\\end{array}\\right)(y-\\theta^Tx)\\\\\n",
    "&=-x(y-\\theta^Tx)\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Average gradient:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla\\mathcal L_n(\\theta)&=\\frac{1}{n}\\sum_{(x,y)\\in S_n}-x(y-\\theta^Tx)\\\\\n",
    "&= \\frac{1}{n}(-x_1(y_1-\\theta^Tx_1)-x_2(y_2-\\theta^Tx_2)+...-x_n(y_n-\\theta^Tx_n))\\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^n-x_iy_i+x_ix_i^T\\theta\\\\\n",
    "&=\\frac{1}{n}(X^TX\\theta -X^TY)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Solution:\n",
    "- Exact solution: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{n}(X^TX\\hat\\theta -X^TY) &= 0\\\\\n",
    "X^TX\\hat\\theta &=X^TY\\\\\n",
    "\\hat\\theta&=(X^TX)^{-1}X^TY\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Gradient Descent: $\\theta\\leftarrow\\theta-\\eta_k\\left[\\frac{1}{n}(X^TX\\theta-X^TY)\\right]$\n",
    "\n",
    "### Test Loss (**Actual Data**):\n",
    "\n",
    " $\\mathcal R(\\theta)=\\frac{1}{n}\\sum_{x,y}\\frac{1}{2}(y-\\theta^Tx)^2$\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "### Problem:\n",
    "Same as Linear Regression\n",
    "\n",
    "### Model:\n",
    "Same as Linear Regression with Constant Feature Trick\n",
    "\n",
    "### Optimization:\n",
    "- Point loss: $\\mathcal L_{1,\\lambda}(\\theta)=\\frac{1}{2}(y-\\theta^Tx)^2\\boldsymbol{+\\frac{\\lambda}{2}{\\left\\lVert\\theta\\right\\rVert}^2}$\n",
    "- Average loss: $\\mathcal L_{n,\\lambda}(\\theta)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\frac{1}{2}(y-\\theta^Tx)^2+\\frac{\\lambda}{2}{\\left\\lVert\\theta\\right\\rVert}^2$\n",
    "- Point gradient:\n",
    "$$\n",
    "\\begin{array}\n",
    "\\nabla\\mathcal L_{1,\\lambda}&= \\frac{d}{d\\theta}(\\frac{1}{2}(y-\\theta^Tx)^2+\\frac{\\lambda}{2}{\\left\\lVert\\theta\\right\\rVert}^2)\\\\\n",
    "&= -x(y-\\theta^Tx)+\\lambda\\theta\n",
    "\\end{array}\n",
    "$$\n",
    "- Average gradint:\n",
    "$$\n",
    "\\begin{array}\n",
    "\\nabla \\mathcal L_{n,\\lambda}&=\\frac{d}{d\\theta}(\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\frac{1}{2}(y-\\theta^Tx)^2+\\frac{\\lambda}{2}{\\left\\lVert\\theta\\right\\rVert}^2)\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=0}^n-x_i(y_i-\\theta^Tx_i)+\\lambda\\theta\\\\\n",
    "&=\\frac{1}{n}(X^TX\\theta-X^TY)+\\lambda\\theta\n",
    "\\end{array}\n",
    "$$\n",
    "### Solution:\n",
    "- Exact solution:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{n}(X^TX\\hat\\theta-X^TY)+\\lambda\\hat\\theta&=0\\\\\n",
    "X^TX\\hat\\theta+n\\lambda\\hat\\theta&=X^TY\\\\\n",
    "\\hat\\theta(X^TX+n\\lambda I)&=X^TY\\\\\n",
    "\\hat\\theta&=(X^TX+n\\lambda I)^{-1}X^TY\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Gradient descent: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta&\\leftarrow\\theta-\\eta_k(\\frac{1}{n}(X^TX\\theta-X^TY)+\\lambda\\theta)\\\\\n",
    "\\theta&\\leftarrow\\theta-\\eta_k(\\frac{1}{n}(X^TX\\theta-X^TY))-\\eta_k\\lambda\\theta\\\\\n",
    "\\theta&\\leftarrow(1-\\eta_k\\lambda)\\theta-\\eta_k(\\frac{1}{n}(X^TX\\theta-X^TY))\n",
    "\\end{aligned}\n",
    "$$\n",
    "*Note: Always Invertible*\n",
    "\n",
    "### Test Loss:\n",
    "\n",
    "$\\mathcal R(\\theta)=\\frac{1}{n}\\sum_{x,y}\\frac{1}{2}(y-\\theta^Tx)^2$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification (Perceptrons)\n",
    "\n",
    "### Problem:\n",
    "\n",
    "- Input: matrix\n",
    "$$\n",
    "x=\\left(\\begin{array}{cccc}x_{11}&x_{21}&...&x_{d1}\\\\x_{12}&x_{22}&...&x_{d2}\\\\...&...&...&...\\\\x_{1n}&x_{2n}&...&x_{dn}\\end{array}\\right)\n",
    "$$\n",
    "- Output: vector $y:\\{y_1,y_2,...,y_d\\}$\n",
    "- x can take any numerical value, $y_i\\in\\{-1,+1\\}$\n",
    "- $x\\in\\mathbb R^d, y\\in\\mathbb R$\n",
    "\n",
    "### Model\n",
    "\n",
    "$$\\begin{aligned}h(x;\\theta,\\theta_0)&=\\text{sign}(\\theta_1x_1+\\theta_2x_2+...+\\theta_dx_d+\\theta_0)\\\\&=\\text{sign}(\\theta_1x_1+\\theta_2x_2+...+\\theta_dx_d+\\theta_0x_0)\\\\&=\\text{sign}(\\tilde\\theta^T\\tilde x)\\end{aligned}$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x&\\leftarrow\\tilde x\\\\\n",
    "\\theta&\\leftarrow\\tilde\\theta\n",
    "\\end{aligned}\n",
    "$$\n",
    "- parameters: $\\theta\\in\\mathbb R^{d+1}$\n",
    "\n",
    "### Optimization:\n",
    "\n",
    "- Loss function: $\\text{Loss}(z)=\\text{Ind}[z\\leq0]$\n",
    "    - $\\text{Ind}[x]=\\left\\{\\begin{array}{ll}1&x=\\text{True}\\\\0&x=\\text{False}\\end{array}\\right.$\n",
    "- Point loss: $\\mathcal L_1(\\theta;x,y)=\\text{Ind}[y(\\theta^Tx)\\leq0]$\n",
    "    - $y\\neq h(x;\\theta)\\ \\ \\Rightarrow\\ \\ y(\\theta^Tx)\\leq0$\n",
    "    - *Note: misclassification and boundary return same results*\n",
    "- Average loss: $\\mathcal L_n(\\theta;S_n)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\mathcal L_1(\\theta;x,y)$\n",
    "- *note: gradient is 0 almost everywhere, gradient descent impossible.*\n",
    "\n",
    "## Solution:\n",
    "\n",
    "**Perceptron Algorithm (Mistake-Driven Algorithm)**\n",
    "- Initialize $\\theta=0$\n",
    "- For each data $(x,y)\\in S_n$\n",
    "    - if $y(\\theta^Tx)\\leq0$, \n",
    "        - $\\theta\\leftarrow\\theta+yx$\n",
    "- Repeat Step 2 until no mistakes are found\n",
    "- *note: algorithm never terminates if not linearly seperable.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge Loss (Support Vector Machine)\n",
    "\n",
    "### Problem\n",
    "\n",
    "Same as Linear Classification\n",
    "\n",
    "### Model\n",
    "\n",
    "Same as Linear Classification with Constant Feature Trick\n",
    "\n",
    "### Optimization\n",
    "\n",
    "- Loss function: $\\text{Loss}(z)=\\text{max}\\{1-z,0\\}$\n",
    "- Point loss: $\\mathcal L_1(\\theta)=\\text{max}\\{1-y(\\theta^Tx),0\\}$ \n",
    "- Average loss: $\\mathcal L_n(\\theta)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\text{max}\\{1-y(\\theta^Tx),0\\}$\n",
    "- Point gradient: $\\nabla\\mathcal L_1(\\theta)=\\left\\{\\begin{array}{ll}0&\\text{if }z>1\\\\-yx&\\text{otherwise}\\end{array}\\right.$\n",
    "\n",
    "### Solution\n",
    "\n",
    "**Stochastic Gradient Descent**\n",
    "- Initialize $\\theta = 0$\n",
    "- Select data $(x,y)\\in S_n$ **at random**\n",
    "    - if $y(\\theta^Tx)\\leq1$,\n",
    "        - $\\theta\\leftarrow\\theta+\\eta_kyx$\n",
    "- Repeat Step 2 until convergence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Problem:\n",
    "\n",
    "Same as Linear Classification\n",
    "\n",
    "### Model:\n",
    "\n",
    "- $\\begin{aligned}h(x;\\theta)&=\\mathbb P(y=+1|\\ x)\\\\&=\\text{Sig}(\\theta^Tx)\\end{aligned}$\n",
    "    - $\\text{Sig}(z)=\\frac{1}{1+e^{-z}}$\n",
    "    - $\\text{Sig}(z)\\in[0,1]$\n",
    "- $\\text{Sig}(\\theta^Tx)\\geq0.5\\Rightarrow\\theta^Tx\\geq0$\n",
    "- $\\text{Sig}(\\theta^Tx)<0.5\\Rightarrow\\theta^Tx<0$\n",
    "- *Note: *$\\theta^Tx=0$* is the decision boundary.*\n",
    "- $\\mathbb P(y|x)=\\text{Sig}(y(\\theta^Tx))$ for $y\\in\\{-1,+1\\}$\n",
    "\n",
    "### Optimization:\n",
    "- Loss function: $\\text{Loss}(z)=\\log(1+e^{-z})$\n",
    "- Point loss: $\\mathcal L_1(\\theta;x,y)=\\log(1+e^{-y(\\theta^Tx)})$\n",
    "- Average loss:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal L_n(\\theta;S_n)&=-\\frac{1}{n}\\log\\prod_{(x,y)\\in S_n}\\mathbb P(y|x)\\\\\n",
    "&= -\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\log\\frac{1}{1+e^{-y(\\theta^Tx)}}\\\\\n",
    "&=\\frac{1}{n}\\sum_{(x,y)\\in S_n}\\log(1+e^{-y(\\theta^Tx)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Point gradient:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla\\mathcal L_1(\\theta;x,y)&=\\frac{d}{d\\theta}(\\log(1+e^{-y(\\theta^Tx)}))\\\\\n",
    "&=\\frac{-yxe^{-y(\\theta^Tx)}}{1+e^{-y(\\theta^Tx)}}\\\\\n",
    "&=\\frac{-yx}{1+e^{y(\\theta^Tx)}}\\\\\n",
    "&=-yx\\text{Sig}(-y(\\theta^Tx))\\\\\n",
    "&=\\left\\{\\begin{array}{ll}x(\\text{Sig}(\\theta^Tx)-1)&\\text{if }y=+1\\\\x(\\text{Sig}(\\theta^Tx))&\\text{if }y=-1\\end{array}\\right.\\\\\n",
    "&=x(h(x;\\theta)-\\text{Ind}[y=1])\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Average gradient: $\\nabla\\mathcal L_n(\\theta;S_n)=\\frac{1}{n}\\sum_{(x,y)\\in S_n}x(h(x;\\theta)-\\text{Ind}[y=1])$\n",
    "\n",
    "### Solution\n",
    "\n",
    "- Gradient descent: $\\theta\\leftarrow\\theta-\\frac{\\eta_k}{n}\\sum_{(x,y)\\in S_n}x(h(x;\\theta)-\\text{Ind}[y=1])$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "### Problem:\n",
    "\n",
    "- Input: matrix\n",
    "$$\n",
    "x=\\left(\\begin{array}{cccc}x_{11}&x_{21}&...&x_{d1}\\\\x_{12}&x_{22}&...&x_{d2}\\\\...&...&...&...\\\\x_{1n}&x_{2n}&...&x_{dn}\\end{array}\\right)\n",
    "$$\n",
    "- Output: vectors $C_1,C2,...,C_k\\subset\\{1,2,...,n\\}$\n",
    "- x can take any numerical value, $C_i$ are mutually exclusive sets of n\n",
    "\n",
    "- $x\\in\\mathbb R^d, C_i\\in\\mathbb R\\ \\ \\forall i$\n",
    "\n",
    "### Optimization:\n",
    "\n",
    "- Average loss: \n",
    "$\\begin{aligned}\\mathcal L_{n,k}(C_1,...,C_n;S_n)&=\\sum_{j=1}^n\\sum_{i\\in C_j}\\left\\lVert x_i-\\frac{1}{|C_j|}\\sum_{i'\\in C_j}x_{i'}\\right\\rVert^2\\\\\n",
    "\\mathcal L_{n,k}(C_1,...,C_n,z_1,...,z_n;S_n)&=\\sum_{j=1}^n\\sum_{i\\in C_j}\\left\\lVert x_i-z_j\\right\\rVert^2\n",
    "\\end{aligned}$\n",
    "\n",
    "\n",
    "### Solution:\n",
    "\n",
    "**Coordinate Descent**\n",
    "- Initialize centroids $z_1,z_2,...,z_k$\n",
    "- For each $j\\in\\{1,2,...,k\\}$,\n",
    "    - $C_j=\\{\\text{all }i\\text{ such that }x^i\\text{ is closest to }z_j\\}$\n",
    "- For each new cluster,\n",
    "    - $z_j\\leftarrow\\frac{1}{|C_j|}\\sum_{i\\in C_j}x_i$\n",
    "- Repeat Step 2 and Step 3 until convergence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Lu Wei\n",
    "\n",
    "## Overview\n",
    "\n",
    "- Hidden Markov Model (HMM)\n",
    "- Bayesian Network\n",
    "- Reinforcement Learning (RL)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Model (HMM)\n",
    "\n",
    "### Problem\n",
    "\n",
    "- Input: vector $x:\\{x_1,x_2,...,x_n\\}$\n",
    "- Output: vector $y:\\{y_1,y_2,...,y_d\\}$\n",
    "- Where $x$ is the observation and $y$ is the hidden state\n",
    "- $x,y$ is usually categorical, but can be numerical\n",
    "- $x\\in\\mathbb R,y\\in\\mathbb R$\n",
    "\n",
    "### Model\n",
    "\n",
    "### Optimization\n",
    "\n",
    "### Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
